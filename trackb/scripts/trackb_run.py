#!/usr/bin/env python3
"""
Track B Main Execution Script
Track B ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/trackb_run.py --mode from_artifacts
    python scripts/trackb_run.py --mode quick --skip_figures
    python scripts/trackb_run.py --help
"""

import argparse
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
TRACKB_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(SCRIPT_DIR))

from common.io import PathResolver, save_json_safe, save_csv_safe
from common.report import ReportGenerator
from integration.pipeline import TrackBPipeline
from integration.step1_loader import Step1Loader
from integration.step2_loader import Step2Loader
from integration.step3_loader import Step3Loader
from validation.ground_truth_validator import GroundTruthValidator
from validation.statistical_validator import run_full_validation, format_validation_report
import json
from visualization.cost_charts import plot_cost_comparison, plot_cost_breakdown
from visualization.performance_charts import plot_detection_performance, plot_confusion_matrices
from visualization.agent_charts import create_all_agent_charts

# ë¡œê¹… ì„¤ì •
def setup_logging(verbose: bool = False) -> None:
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    
    # Run IDê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë””ë ‰í† ë¦¬ ì‚¬ìš©, ì—†ìœ¼ë©´ ì„ì‹œ ê²½ë¡œ
    log_file = TRACKB_ROOT / 'outputs' / 'trackb_run.log'
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, encoding='utf-8')
        ]
    )

logger = logging.getLogger(__name__)


def load_config(config_path: Path) -> dict:
    """ì„¤ì • ë¡œë“œ. ì¶œë ¥ ê²½ë¡œë¥¼ trackb ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ê²€ì¦ê¸°ì™€ ì¼ì¹˜ì‹œí‚¨ë‹¤."""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    # ì¶œë ¥ì€ í•­ìƒ trackb/outputs/ ì— ë‘ì–´ verify_outputs.pyì™€ ë™ì¼í•œ ê¸°ì¤€ ì‚¬ìš©
    config.setdefault('paths', {})['trackb_outputs'] = str(TRACKB_ROOT / 'outputs')
    return config


def run_from_artifacts(config: dict, skip_figures: bool = False) -> dict:
    """
    ì•„í‹°íŒ©íŠ¸ì—ì„œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        skip_figures: Trueë©´ ê·¸ë¦¼ ìƒì„± ìŠ¤í‚µ
    
    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("=" * 60)
    logger.info("Track B íŒŒì´í”„ë¼ì¸ ì‹œì‘ (mode: from_artifacts)")
    logger.info("=" * 60)
    
    start_time = datetime.now()
    
    # Run ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
    run_id = start_time.strftime("%Y%m%d_%H%M%S")
    logger.info(f"Run ID: {run_id}")
    
    # Run IDë¥¼ configì— ì¶”ê°€
    config['run_id'] = run_id
    config['run_timestamp'] = start_time.isoformat()
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = TrackBPipeline(config)
    
    # ì‹¤í–‰
    results = pipeline.run()
    
    # ê·¸ë¦¼ ìƒì„±
    if not skip_figures:
        logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
        figures_dir = Path(config['paths']['trackb_outputs']) / 'figures'
        figures_dir = TRACKB_ROOT / figures_dir
        figures_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # ë¹„ìš© ë¹„êµ
            if 'validation' in results and 'comparison' in results['validation']:
                comparison_df = results['validation']['comparison']
                plot_cost_comparison(
                    comparison_df,
                    figures_dir / 'cost_comparison.png'
                )
                plot_detection_performance(
                    comparison_df,
                    figures_dir / 'detection_performance.png'
                )
            
            # Agent ì°¨íŠ¸
            if 'agent' in results:
                agent_results = results['agent']
                
                optimizer_history = None
                best_config = {'tau0': 0.6, 'tau1': 0.6, 'tau2a': 0.6}
                
                if agent_results.get('optimizer'):
                    optimizer_history = agent_results['optimizer'].get('history_df')
                    if 'summary' in agent_results['optimizer']:
                        best_config = agent_results['optimizer']['summary'].get(
                            'best_config', best_config
                        )
                
                scheduler_log = None
                if agent_results.get('scheduler'):
                    scheduler_log = agent_results['scheduler'].get('log_df')
                
                explainer_trace = None
                if agent_results.get('explainer'):
                    explainer_trace = agent_results['explainer'].get('trace_df')
                
                if optimizer_history is not None or scheduler_log is not None:
                    create_all_agent_charts(
                        optimizer_history,
                        best_config,
                        scheduler_log,
                        explainer_trace,
                        figures_dir
                    )
            
            logger.info(f"âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ: {figures_dir}")
        except Exception as e:
            logger.warning(f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    # Run-isolated paths
    run_id = config.get('run_id', 'unknown')
    results['run_id'] = run_id
    run_output_dir = TRACKB_ROOT / 'outputs' / f'run_{run_id}'
    reports_dir = run_output_dir / 'reports'
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    # Lot leakage ì§„ë‹¨ + Random seed sweep (paper_reports / final_validationì—ì„œ ì‚¬ìš©)
    try:
        from lot_leakage_diagnostics import compute_lot_diagnostics
        raw = config.get('paths', {}).get('step1_artifacts', '')
        step1_dir = (TRACKB_ROOT / raw).resolve() if raw else TRACKB_ROOT.parent / 'data' / 'step1'
        step1_dir = step1_dir if step1_dir.exists() else None
        diag = compute_lot_diagnostics(run_output_dir, step1_dir=step1_dir)
        out_path = run_output_dir / 'validation' / 'lot_leakage_diagnostics.json'
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(diag, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… Lot leakage diagnostics: {out_path}")
    except Exception as e:
        logger.warning(f"Lot leakage diagnostics ì˜¤ë¥˜: {e}")
    try:
        from random_seed_sweep import run_sweep
        sweep_df, sweep_summary = run_sweep(run_output_dir, n_seeds=50)
        val_dir = run_output_dir / 'validation'
        val_dir.mkdir(parents=True, exist_ok=True)
        sweep_df.to_csv(val_dir / 'random_seed_sweep.csv', index=False)
        with open(val_dir / 'random_seed_sweep_summary.json', 'w', encoding='utf-8') as f:
            json.dump(sweep_summary, f, indent=2, ensure_ascii=False)
        logger.info("âœ… Random seed sweep: validation/random_seed_sweep.csv, random_seed_sweep_summary.json")
    except Exception as e:
        logger.warning(f"Random seed sweep ì˜¤ë¥˜: {e}")
    
    # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¨¼ì € ìƒì„± (ë³´ê³ ì„œì—ì„œ SHA256Â·ì¦ê±° ì¸ë±ìŠ¤ ì°¸ì¡°)
    logger.info("ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
    try:
        manifest = pipeline.generate_manifest()
        results['manifest'] = manifest
    except Exception as e:
        logger.warning(f"ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë³´ê³ ì„œ ìƒì„± (í˜„ì¬ run ë°ì´í„°ë§Œ ì‚¬ìš©, SHA256 í¬í•¨)
    logger.info("ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    try:
        generate_master_report(results, config, reports_dir, run_id=run_id, run_output_dir=run_output_dir)
    except Exception as e:
        logger.warning(f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    # Paper Input Bundle: paper_bundle.json + Core/Appendix/TrackA/PAPER_IO_TRACE
    logger.info("Paper bundle ìƒì„± ì¤‘...")
    try:
        from paper_bundle import write_paper_bundle
        from paper_reports import write_paper_reports
        write_paper_bundle(run_output_dir, config, run_id)
        write_paper_reports(run_output_dir, run_id, config)
    except Exception as e:
        logger.warning(f"Paper bundle/reports ì˜¤ë¥˜: {e}")

    # ê¶ê·¹ ëª©í‘œ ì¢…í•© ê²€ì¦: FINAL_VALIDATION.md (Q1~Q6, íŒì •) â€” ì‚°ì¶œë¬¼ ê¸°ë°˜ë§Œ ì‚¬ìš©
    logger.info("Final validation ìƒì„± ì¤‘...")
    try:
        from final_validation import write_final_validation
        write_final_validation(run_output_dir, run_id)
    except Exception as e:
        logger.warning(f"Final validation ì˜¤ë¥˜: {e}")

    # ì™„ë£Œ
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("=" * 60)
    logger.info(f"Track B íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    logger.info(f"ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
    logger.info("=" * 60)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print_summary(results)
    
    return results


def generate_master_report(
    results: dict,
    config: dict,
    reports_dir: Path,
    run_id: str = 'unknown',
    run_output_dir: Optional[Path] = None,
) -> Path:
    """ë§ˆìŠ¤í„° ë³´ê³ ì„œ ìƒì„±. í˜„ì¬ run ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë©°, SHA256Â·ì¦ê±° ì¸ë±ìŠ¤ í¬í•¨."""
    if run_output_dir is None:
        run_output_dir = reports_dir.parent
    report_gen = ReportGenerator(reports_dir, language='korean')
    
    # í—¤ë”
    report_gen.add_header(
        "Track B ê²€ì¦ ë³´ê³ ì„œ",
        "AI ê¸°ë°˜ ëª¨ë“ˆëŸ¬ í”„ë ˆì„ì›Œí¬ - ê³¼í•™ì  ê²€ì¦"
    )
    # í˜„ì¬ runë§Œ ì‚¬ìš© ëª…ì‹œ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
    report_gen.add_run_disclaimer(run_id)
    
    # ìš”ì•½
    if 'validation' in results and 'comparison' in results['validation']:
        comparison_df = results['validation']['comparison']
        
        summary_content = """
### ì£¼ìš” ê¸°ì—¬ (Ground Truth ê²€ì¦)
- ë°ì´í„°: 200ê°œ ì‹¤ì œ ì›¨ì´í¼ (yield_true ìˆìŒ)
- ë°©ë²•: ììœ¨ ìµœì í™” Agent
- Baseline: Random (10%) + Rule-based (top 10%)

"""
        report_gen.add_section("ìš”ì•½", summary_content)
        # ì •ì±… ë°©ì–´: Primaryë§Œ ê²°ë¡ , ë‚˜ë¨¸ì§€ exploratory
        report_gen.sections.append("""**Primary conclusions use only**: Recall@10% and normalized cost reduction (%) with bootstrap 95% CI.
**All other tests** (t-test, chi-square, McNemar) are exploratory and not used for final claims.

""")
        # ë¹„ìš© = ì •ê·œí™” ë‹¨ìœ„ ëª…ì‹œ (ëˆì²˜ëŸ¼ ì˜¤í•´ ë°©ì§€)
        report_gen.sections.append("Cost values are reported in **normalized units (unitless)**. No currency or absolute money is used.\n")
        report_gen.sections.append("Numbers like 3000/150/500 in tables are **normalized units (not currency)**.\n\n")
        # í‘œ ì»¬ëŸ¼ëª…: ë³´ê³ ì„œì—ì„œë§Œ _norm ì ‘ë¯¸ì‚¬ë¡œ ë¼ë²¨ë§ (ì ˆëŒ€ ë¹„ìš© ì˜¤í•´ ë°©ì§€)
        display_df = comparison_df.rename(columns={
            'n_sem': 'n_followup',
            'sem_cost': 'followup_cost_norm',
            'cost_sem_unit': 'followup_unit_norm',
            'inline_cost': 'inline_cost_norm',
            'total_cost': 'total_cost_norm',
            'cost_per_catch': 'cost_per_catch_norm',
            'cost_inline_unit': 'cost_unit_inline_norm',
        }, errors='ignore')
        report_gen.add_comparison_table(display_df)
    
    # ê²€ì¦ ìƒíƒœ
    report_gen.add_validation_status()
    
    # í†µê³„ ê²€ì¦ (í˜„ì¬ runì˜ statistical_tests.jsonì—ì„œ ì§ì ‘ ì½ê¸°)
    stats_json_path = reports_dir.parent / 'validation' / 'statistical_tests.json'
    if stats_json_path.exists():
        try:
            with open(stats_json_path, 'r', encoding='utf-8') as f:
                stats_data = json.load(f)
            stats_section = format_statistical_tests_from_json(stats_data)
            report_gen.sections.append(stats_section)
        except Exception as e:
            logger.warning(f"í†µê³„ ê²€ì¦ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            report_gen.add_section("í†µê³„ ê²€ì¦ ê²°ê³¼", f"âš ï¸ í†µê³„ ê²€ì¦ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    elif 'validation' in results:
        stats_section = format_validation_report(results['validation'])
        report_gen.sections.append(stats_section)
    
    # í•œê³„
    report_gen.add_limitations()
    
    # ì¦ê±° ì¸ë±ìŠ¤ (SHA256Â·ê²½ë¡œ)
    manifest_path = run_output_dir / '_manifest.json'
    manifest_dict = None
    if manifest_path.exists():
        try:
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_dict = json.load(f)
        except Exception as e:
            logger.warning(f"Manifest ë¡œë“œ ì‹¤íŒ¨: {e}")
    report_gen.add_evidence_index(run_id, manifest_path=manifest_path, manifest_dict=manifest_dict)
    
    # ì¬í˜„ì„± (í˜„ì¬ run ê¸°ì¤€ ê²½ë¡œ)
    report_gen.add_reproducibility_section(
        run_id=run_id,
        manifest_path='../_manifest.json',
        config_path='../../configs/trackb_config.json'
    )
    
    # ì €ì¥
    output_path = report_gen.generate("trackB_report.md")
    
    logger.info(f"âœ… ë§ˆìŠ¤í„° ë³´ê³ ì„œ ìƒì„±: {output_path}")
    
    return output_path


def format_statistical_tests_from_json(stats_data: dict) -> str:
    """statistical_tests.jsonì—ì„œ ì§ì ‘ í†µê³„ ê²€ì¦ ê²°ê³¼ í¬ë§·íŒ…"""
    report = """
## í†µê³„ ê²€ì¦ ê²°ê³¼

### ê²€ì • ìš”ì•½

"""
    
    tests = stats_data.get('tests', {})
    summary = stats_data.get('summary', {})
    
    # T-test
    if 't_test_yields' in tests:
        t_test = tests['t_test_yields']
        t_sig = "âœ… ìœ ì˜" if t_test.get('significant_005') else "âŒ ë¹„ìœ ì˜"
        t_p = t_test.get('p_value', 0)
        t_p_str = "<0.001" if t_p < 0.001 else f"{t_p:.4f}"
        report += f"- **T-test (yield ë¹„êµ)**: t={t_test.get('t_statistic', 'N/A'):.3f}, p={t_p_str} ({t_sig})\n"
        report += f"  - ë¹„êµ ì§‘ë‹¨: ì„ íƒëœ ì›¨ì´í¼ì˜ yield_true ë¶„í¬ (baseline vs framework)\n"
        report += f"  - Baseline mean: {t_test.get('baseline_mean', 0):.4f}, Framework mean: {t_test.get('framework_mean', 0):.4f}\n"
        report += f"  - Sample sizes: n_baseline={t_test.get('n_baseline', 0)}, n_framework={t_test.get('n_framework', 0)}\n"
    
    # Chi-square
    if 'chi_square_detection' in tests:
        chi_sq = tests['chi_square_detection']
        chi_sig = "âœ… ìœ ì˜" if chi_sq.get('significant_005') else "âŒ ë¹„ìœ ì˜"
        chi_p = chi_sq.get('p_value', 0)
        chi_p_str = "<0.001" if chi_p < 0.001 else f"{chi_p:.4f}"
        report += f"- **Chi-square (ê²€ì¶œë¥ )**: Ï‡Â²={chi_sq.get('chi2_statistic', 'N/A'):.3f}, p={chi_p_str} ({chi_sig})\n"
        contingency = chi_sq.get('contingency_table', [])
        if contingency:
            report += f"  - Contingency table: Baseline [TP={contingency[0][0]}, FN={contingency[0][1]}], Framework [TP={contingency[1][0]}, FN={contingency[1][1]}]\n"
    
    # Bootstrap: policy â€” % CI only, no absolute cost CI
    if 'bootstrap_cost' in tests:
        bootstrap = tests['bootstrap_cost']
        pct_low = bootstrap.get('delta_cost_pct_ci_lower')
        pct_high = bootstrap.get('delta_cost_pct_ci_upper')
        if pct_low is not None and pct_high is not None:
            report += f"- **Bootstrap (normalized cost reduction %, 95% CI)**: [{pct_low:.1f}%, {pct_high:.1f}%] (percentage only; no absolute money units)\n"
        else:
            report += f"- **Bootstrap (normalized cost reduction %)**: observed {bootstrap.get('percent_reduction', 0):.1f}% (CI in percentage only)\n"
        report += f"  - n_bootstrap: {bootstrap.get('n_bootstrap', 0)}\n"
    
    # McNemar
    if 'mcnemar' in tests:
        mcnemar = tests['mcnemar']
        mcn_sig = "âœ… ìœ ì˜" if mcnemar.get('significant_005') else "âŒ ë¹„ìœ ì˜"
        mcn_p = mcnemar.get('p_value', 0)
        mcn_p_str = "<0.001" if mcn_p < 0.001 else f"{mcn_p:.4f}"
        report += f"- **McNemar**: statistic={mcnemar.get('statistic', 'N/A'):.3f}, p={mcn_p_str} ({mcn_sig})\n"
    
    # High-risk count í™•ì¸
    hr_count = stats_data.get('high_risk_count', None)
    if hr_count is not None:
        report += f"\n**High-risk count**: {hr_count} (í•˜ìœ„ 20% ì •ì˜ ê¸°ì¤€)\n"
    
    # ê²°ë¡ 
    report += f"""
### ì „ì²´ ê²°ë¡ 

{summary.get('conclusion', '')}

- ì´ ê²€ì • ìˆ˜: {summary.get('total_tests', 0)}
- ìœ ì˜í•œ ê²€ì • ìˆ˜: {summary.get('significant_count', 0)}
- ìœ ì˜í•œ ê²€ì •: {', '.join(summary.get('significant_tests', []))}
"""
    
    return report


def print_summary(results: dict) -> None:
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    # ë² ì´ìŠ¤ë¼ì¸ ê²°ê³¼
    if 'baselines' in results:
        print("\nğŸ“Š Baseline ê²°ê³¼:")
        for name, data in results['baselines'].items():
            if name == 'comparison':
                continue
            if isinstance(data, dict) and 'metrics' in data:
                metrics = data['metrics']
                print(f"  {name}:")
                print(f"    - Recall: {metrics.get('high_risk_recall', 0):.1%}")
                print(f"    - total_cost (normalized): {metrics.get('total_cost', 0):,.0f}")
    
    # Agent ê²°ê³¼
    if 'agent' in results:
        agent = results['agent']
        print("\nğŸ¤– Agent ê²°ê³¼:")
        
        if agent.get('optimizer'):
            opt = agent['optimizer']
            print(f"  Optimizer: tau0={opt.get('best_tau0', 'N/A')}, score={opt.get('best_score', 0):.4f}")
        
        if agent.get('scheduler'):
            sched = agent['scheduler']['summary']
            print(f"  Scheduler: {sched.get('n_inspected', 0)}/{sched.get('total_wafers', 0)} ì„ íƒ")
            print(f"  ì˜ˆì‚° ì‚¬ìš©: {sched.get('budget_utilization', 0):.1%}")
        
        if agent.get('framework_metrics'):
            fm = agent['framework_metrics']
            print(f"  Framework ì„±ëŠ¥:")
            print(f"    - Recall: {fm['detection'].get('high_risk_recall', 0):.1%}")
            print(f"    - Precision: {fm['detection'].get('high_risk_precision', 0):.1%}")
            print(f"    - total_cost (normalized): {fm['cost'].get('total_cost', 0):,.0f}")
    
    # í†µê³„ ê²€ì¦
    if 'validation' in results:
        validation = results['validation']
        print("\nğŸ“ˆ í†µê³„ ê²€ì¦:")
        
        for comp_name, comp_data in validation.get('comparisons', {}).items():
            print(f"  {comp_name}:")
            
            chi_sq = comp_data.get('chi_square', {})
            if chi_sq.get('p_value') is not None:
                sig = "âœ…" if chi_sq.get('significant_005') else "âŒ"
                print(f"    - Chi-square: p={chi_sq['p_value']:.4f} {sig}")
            
            bootstrap = comp_data.get('bootstrap', {})
            if bootstrap.get('percent_reduction') is not None:
                print(f"    - ë¹„ìš© ì ˆê°: {bootstrap['percent_reduction']:.1f}%")
    
    run_id = results.get('run_id', 'unknown')
    report_path = f"trackb/outputs/run_{run_id}/reports/trackB_report.md"
    print("\n" + "=" * 60)
    print(f"ìƒì„¸ ê²°ê³¼: {report_path}")
    print("=" * 60 + "\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='Track B ê³¼í•™ì  ê²€ì¦ íŒŒì´í”„ë¼ì¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # ì•„í‹°íŒ©íŠ¸ì—ì„œ ì „ì²´ ì‹¤í–‰
    python scripts/trackb_run.py --mode from_artifacts
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê·¸ë¦¼ ìŠ¤í‚µ)
    python scripts/trackb_run.py --mode from_artifacts --skip_figures
    
    # ìƒì„¸ ë¡œê·¸
    python scripts/trackb_run.py --mode from_artifacts --verbose
        """
    )
    
    parser.add_argument(
        '--mode',
        choices=['from_artifacts', 'quick', 'full'],
        default='from_artifacts',
        help='ì‹¤í–‰ ëª¨ë“œ (ê¸°ë³¸: from_artifacts)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='configs/trackb_config.json',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--skip_figures',
        action='store_true',
        help='ê·¸ë¦¼ ìƒì„± ìŠ¤í‚µ'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.verbose)
    
    # ì„¤ì • ë¡œë“œ
    config_path = TRACKB_ROOT / args.config
    if not config_path.exists():
        logger.error(f"ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        sys.exit(1)
    
    config = load_config(config_path)
    
    # ëª¨ë“œë³„ ì‹¤í–‰
    if args.mode == 'from_artifacts':
        results = run_from_artifacts(config, skip_figures=args.skip_figures)
    elif args.mode == 'quick':
        results = run_from_artifacts(config, skip_figures=True)
    elif args.mode == 'full':
        results = run_from_artifacts(config, skip_figures=False)
    else:
        logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {args.mode}")
        sys.exit(1)
    
    # ì¢…ë£Œ
    logger.info("Track B íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")


if __name__ == '__main__':
    main()
